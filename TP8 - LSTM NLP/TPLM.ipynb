{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"colab":{"name":"TPLM.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"vD0irIolPbn0","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EImdv5dpQZEg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"019be3ca-cf1e-4fe6-bf34-480674c92fb2","executionInfo":{"status":"ok","timestamp":1583749267858,"user_tz":-60,"elapsed":2504,"user":{"displayName":"Thomas KOCH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ87m3muD8rwgrNqbB_aT6SAfORhzKUeCrE4YIZA=s64","userId":"03343798980471844337"}}},"source":["torch.cuda.is_available()"],"execution_count":70,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":70}]},{"cell_type":"code","metadata":{"id":"wrblLcFuQgkB","colab_type":"code","colab":{}},"source":["# # let us run this cell only if CUDA is available\n","# # We will use ``torch.device`` objects to move tensors in and out of GPU\n","# if torch.cuda.is_available():\n","#     device = torch.device(\"cuda\")          # a CUDA device object\n","#     y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n","#     x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n","#     z = x + y\n","#     print(z)\n","#     print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bhHk8XDpQrE4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"11908b3b-4603-4fc5-a34a-5e962668d92f","executionInfo":{"status":"ok","timestamp":1583749270914,"user_tz":-60,"elapsed":5448,"user":{"displayName":"Thomas KOCH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ87m3muD8rwgrNqbB_aT6SAfORhzKUeCrE4YIZA=s64","userId":"03343798980471844337"}}},"source":["! python --version"],"execution_count":72,"outputs":[{"output_type":"stream","text":["Python 3.6.9\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_7XkNA7sPboC","colab_type":"text"},"source":["### A (very small) introduction to pytorch"]},{"cell_type":"markdown","metadata":{"id":"9TWD23FwPboF","colab_type":"text"},"source":["Pytorch Tensors are very similar to Numpy arrays, with the added benefit of being usable on GPU. For a short tutorial on various methods to create tensors of particular types, see [this link](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py).\n","The important things to note are that Tensors can be created empty, from lists, and it is very easy to convert a numpy array into a pytorch tensor, and inversely.\n","One very important way to manipulate tensors that is different from numpy is the method ```.view``` which is used, as ```reshape```, to change the shape of a tensor. The difference is that ```.view``` will avoid making a copy of the tensor. "]},{"cell_type":"code","metadata":{"id":"_9RY_A6qPboI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"9a30932c-3e33-44d9-f35c-e917e1f460d1","executionInfo":{"status":"ok","timestamp":1583749270917,"user_tz":-60,"elapsed":5392,"user":{"displayName":"Thomas KOCH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ87m3muD8rwgrNqbB_aT6SAfORhzKUeCrE4YIZA=s64","userId":"03343798980471844337"}}},"source":["a = torch.LongTensor(5)\n","b = torch.LongTensor([5])\n","\n","print(a)\n","print(b)"],"execution_count":73,"outputs":[{"output_type":"stream","text":["tensor([         1935100736,                  32,          4294967295,\n","        3630571113557484390, 3617628775105509173])\n","tensor([5])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gjVjCzMXPboR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"8a7ae783-bf28-4a61-905b-9a12f8560d5b","executionInfo":{"status":"ok","timestamp":1583749270923,"user_tz":-60,"elapsed":5337,"user":{"displayName":"Thomas KOCH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ87m3muD8rwgrNqbB_aT6SAfORhzKUeCrE4YIZA=s64","userId":"03343798980471844337"}}},"source":["a = torch.FloatTensor([2])\n","b = torch.FloatTensor([3])\n","\n","print(a + b)"],"execution_count":74,"outputs":[{"output_type":"stream","text":["tensor([5.])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"L-WiyMaAPboa","colab_type":"text"},"source":["The main interest in us using Pytorch is the ```autograd``` package. ```torch.Tensor```objects have an attribute ```.requires_grad```; if set as True, it starts to track all operations on it. When you finish your computation, can call ```.backward()``` and all the gradients are computed automatically (and stored in the ```.grad``` attribute).\n","\n","One way to easily cut a tensor from the computational once it is not needed anymore is to use ```.detach()```.\n","More info on automatic differentiation in pytorch on [this link](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py)."]},{"cell_type":"code","metadata":{"id":"mN-eOGrOPbod","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"2d727052-6da5-48df-9db2-671f334bada3","executionInfo":{"status":"ok","timestamp":1583749270925,"user_tz":-60,"elapsed":5278,"user":{"displayName":"Thomas KOCH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ87m3muD8rwgrNqbB_aT6SAfORhzKUeCrE4YIZA=s64","userId":"03343798980471844337"}}},"source":["x = torch.tensor(1., requires_grad=True)\n","w = torch.tensor(2., requires_grad=True)\n","b = torch.tensor(3., requires_grad=True)\n","\n","# Build a computational graph.\n","y = w * x + b    # y = 2 * x + 3\n","\n","# Compute gradients.\n","y.backward()\n","\n","# Print out the gradients.\n","print(x.grad)    # x.grad = 2 \n","print(w.grad)    # w.grad = 1 \n","print(b.grad)    # b.grad = 1 "],"execution_count":75,"outputs":[{"output_type":"stream","text":["tensor(2.)\n","tensor(1.)\n","tensor(1.)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fqe4wedqPbom","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"d619a59e-93ff-4da5-9c88-bb3a42cd53e2","executionInfo":{"status":"ok","timestamp":1583749270927,"user_tz":-60,"elapsed":5213,"user":{"displayName":"Thomas KOCH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ87m3muD8rwgrNqbB_aT6SAfORhzKUeCrE4YIZA=s64","userId":"03343798980471844337"}}},"source":["x = torch.randn(10, 3)\n","y = torch.randn(10, 2)\n","\n","# Build a fully connected layer.\n","linear = nn.Linear(3, 2)\n","for name, p in linear.named_parameters():\n","    print(name)\n","    print(p)\n","\n","# Build loss function - Mean Square Error\n","criterion = nn.MSELoss()\n","\n","# Forward pass.\n","pred = linear(x)\n","\n","# Compute loss.\n","loss = criterion(pred, y)\n","print('Initial loss: ', loss.item())\n","\n","# Backward pass.\n","loss.backward()\n","\n","# Print out the gradients.\n","print ('dL/dw: ', linear.weight.grad) \n","print ('dL/db: ', linear.bias.grad)"],"execution_count":76,"outputs":[{"output_type":"stream","text":["weight\n","Parameter containing:\n","tensor([[-0.2802,  0.3308,  0.1238],\n","        [-0.3682,  0.1061, -0.5716]], requires_grad=True)\n","bias\n","Parameter containing:\n","tensor([ 0.5730, -0.0342], requires_grad=True)\n","Initial loss:  1.9507572650909424\n","dL/dw:  tensor([[ 0.0426,  0.4599, -0.0240],\n","        [-1.3008, -0.4855, -0.9547]])\n","dL/db:  tensor([0.9605, 0.3847])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RsgWrYfgPbow","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"94ff0477-006d-4632-fb71-b00d7ae68cfe","executionInfo":{"status":"ok","timestamp":1583749270929,"user_tz":-60,"elapsed":5143,"user":{"displayName":"Thomas KOCH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ87m3muD8rwgrNqbB_aT6SAfORhzKUeCrE4YIZA=s64","userId":"03343798980471844337"}}},"source":["# You can perform gradient descent manually, with an in-place update ...\n","linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n","linear.bias.data.sub_(0.01 * linear.bias.grad.data)\n","\n","# Print out the loss after 1-step gradient descent.\n","pred = linear(x)\n","loss = criterion(pred, y)\n","print('Loss after one update: ', loss.item())"],"execution_count":77,"outputs":[{"output_type":"stream","text":["Loss after one update:  1.909816026687622\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HcPCRF2mPbo4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"f79aa2e3-0eaf-4c5e-9489-ab87a74dfaad","executionInfo":{"status":"ok","timestamp":1583749270931,"user_tz":-60,"elapsed":5087,"user":{"displayName":"Thomas KOCH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ87m3muD8rwgrNqbB_aT6SAfORhzKUeCrE4YIZA=s64","userId":"03343798980471844337"}}},"source":["# Use the optim package to define an Optimizer that will update the weights of the model.\n","optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n","\n","# By default, gradients are accumulated in buffers( i.e, not overwritten) whenever .backward()\n","# is called. Before the backward pass, we need to use the optimizer object to zero all of the\n","# gradients.\n","optimizer.zero_grad()\n","loss.backward()\n","\n","# Calling the step function on an Optimizer makes an update to its parameters\n","optimizer.step()\n","\n","# Print out the loss after the second step of gradient descent.\n","pred = linear(x)\n","loss = criterion(pred, y)\n","print('Loss after two updates: ', loss.item())"],"execution_count":78,"outputs":[{"output_type":"stream","text":["Loss after two updates:  1.8700462579727173\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2vftmcLKPbpA","colab_type":"text"},"source":["### Tools for data processing "]},{"cell_type":"code","metadata":{"id":"fRZhWLPAPbpD","colab_type":"code","colab":{}},"source":["import os\n","import time\n","import math\n","from collections import Counter\n","import pprint\n","pp = pprint.PrettyPrinter(indent=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vfW80jpkPbpN","colab_type":"text"},"source":["We create a ```Dictionary``` class, that we are going to use to create a vocabulary for our text data. The goal here is to have a convenient tool, with easy access to any information we could need:\n","- A python dictionary ```word2idx``` allowing easy transformation of tokenized text into indexes\n","- A list ```idx2word```, allowing us to find the word corresponding to an index (for interpretation and generation)\n","- A python dictionary ```counter``` used to build the vocabulary, that can provide us with frequency information if needed. \n","- The ```total``` count of words in the dictionary.\n","\n","Important: The data that we are going to use are already pre-processed so we don't need to create special tokens and control the size of the vocabulary ourselves. However, when the text data is raw, methods to preprocess it conveniently should be added here. "]},{"cell_type":"code","metadata":{"id":"wWp3DjuqPbpQ","colab_type":"code","colab":{}},"source":["class Dictionary(object):\n","    def __init__(self):\n","        self.word2idx = {}\n","        self.idx2word = []\n","        self.counter = {}\n","        self.total = 0\n","\n","    def add_word(self, word):\n","        if word not in self.word2idx:\n","            self.idx2word.append(word)\n","            self.word2idx[word] = len(self.idx2word) - 1\n","            self.counter.setdefault(word, 0)\n","        self.counter[word] += 1\n","        self.total += 1\n","        return self.word2idx[word]\n","\n","    def __len__(self):\n","        return len(self.idx2word)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PV3CwvE0PbpY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":173},"outputId":"bcdbd62e-4d8a-4699-d341-6dacb1c38690","executionInfo":{"status":"ok","timestamp":1583749270938,"user_tz":-60,"elapsed":4936,"user":{"displayName":"Thomas KOCH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ87m3muD8rwgrNqbB_aT6SAfORhzKUeCrE4YIZA=s64","userId":"03343798980471844337"}}},"source":["with open('./train.txt', 'r') as f:\n","    print(f.readline())\n","    print(f.readline())\n","    print(f.readline())\n","    print(f.readline())"],"execution_count":81,"outputs":[{"output_type":"stream","text":[" \n","\n"," = Valkyria Chronicles III = \n","\n"," \n","\n"," Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EWGw9uSyPbpf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"8d9b998e-95f0-43d1-aff5-3c27491a74c9","executionInfo":{"status":"ok","timestamp":1583749270940,"user_tz":-60,"elapsed":4884,"user":{"displayName":"Thomas KOCH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ87m3muD8rwgrNqbB_aT6SAfORhzKUeCrE4YIZA=s64","userId":"03343798980471844337"}}},"source":["# Let's take the four first lines of our training data:\n","corpus = ''\n","with open('./train.txt', 'r') as f:\n","    for i in range(4):\n","        corpus += f.readline()\n","        \n","# Create an empty Dictionary, separate and add all words. \n","dictio = Dictionary()\n","words = corpus.split()\n","for word in words:\n","    dictio.add_word(word)\n","\n","# Take a look at the objects created:\n","pp.pprint(dictio.word2idx) # index du mot dans le dictionnaire\n","print()\n","pp.pprint(dictio.idx2word) # dico\n","print()\n","pp.pprint(dictio.counter) # nombre de fois où le mot apparait\n","print()\n","pp.pprint(dictio.total) # dictionnaire complet"],"execution_count":82,"outputs":[{"output_type":"stream","text":["{'\"': 60,\n"," '(': 9,\n"," ')': 18,\n"," ',': 12,\n"," '.': 14,\n"," '2011': 44,\n"," '3': 6,\n"," ':': 7,\n"," '<unk>': 8,\n"," '=': 0,\n"," '@-@': 29,\n"," 'Battlefield': 17,\n"," 'Chronicles': 2,\n"," 'Europan': 70,\n"," 'Gallia': 67,\n"," 'III': 3,\n"," 'Imperial': 80,\n"," 'January': 43,\n"," 'Japan': 24,\n"," 'Japanese': 10,\n"," 'Media.Vision': 37,\n"," 'Nameless': 61,\n"," 'PlayStation': 39,\n"," 'Portable': 40,\n"," 'Raven': 81,\n"," 'Released': 41,\n"," 'Second': 69,\n"," 'Sega': 35,\n"," 'Senjō': 4,\n"," 'Valkyria': 1,\n"," 'War': 71,\n"," 'a': 26,\n"," 'against': 79,\n"," 'and': 36,\n"," 'are': 77,\n"," 'as': 22,\n"," 'black': 75,\n"," 'by': 34,\n"," 'commonly': 19,\n"," 'developed': 33,\n"," 'during': 68,\n"," 'first': 58,\n"," 'follows': 59,\n"," 'for': 38,\n"," 'fusion': 49,\n"," 'game': 32,\n"," 'gameplay': 52,\n"," 'in': 42,\n"," 'is': 25,\n"," 'it': 45,\n"," 'its': 53,\n"," 'lit': 13,\n"," 'military': 63,\n"," 'nation': 66,\n"," 'no': 5,\n"," 'of': 15,\n"," 'operations': 76,\n"," 'outside': 23,\n"," 'parallel': 57,\n"," 'penal': 62,\n"," 'perform': 73,\n"," 'pitted': 78,\n"," 'playing': 30,\n"," 'predecessors': 54,\n"," 'real': 50,\n"," 'referred': 20,\n"," 'role': 28,\n"," 'runs': 56,\n"," 'same': 48,\n"," 'secret': 74,\n"," 'series': 47,\n"," 'serving': 65,\n"," 'story': 55,\n"," 'tactical': 27,\n"," 'the': 16,\n"," 'third': 46,\n"," 'time': 51,\n"," 'to': 21,\n"," 'unit': 64,\n"," 'video': 31,\n"," 'who': 72,\n"," '戦場のヴァルキュリア3': 11}\n","\n","['=',\n"," 'Valkyria',\n"," 'Chronicles',\n"," 'III',\n"," 'Senjō',\n"," 'no',\n"," '3',\n"," ':',\n"," '<unk>',\n"," '(',\n"," 'Japanese',\n"," '戦場のヴァルキュリア3',\n"," ',',\n"," 'lit',\n"," '.',\n"," 'of',\n"," 'the',\n"," 'Battlefield',\n"," ')',\n"," 'commonly',\n"," 'referred',\n"," 'to',\n"," 'as',\n"," 'outside',\n"," 'Japan',\n"," 'is',\n"," 'a',\n"," 'tactical',\n"," 'role',\n"," '@-@',\n"," 'playing',\n"," 'video',\n"," 'game',\n"," 'developed',\n"," 'by',\n"," 'Sega',\n"," 'and',\n"," 'Media.Vision',\n"," 'for',\n"," 'PlayStation',\n"," 'Portable',\n"," 'Released',\n"," 'in',\n"," 'January',\n"," '2011',\n"," 'it',\n"," 'third',\n"," 'series',\n"," 'same',\n"," 'fusion',\n"," 'real',\n"," 'time',\n"," 'gameplay',\n"," 'its',\n"," 'predecessors',\n"," 'story',\n"," 'runs',\n"," 'parallel',\n"," 'first',\n"," 'follows',\n"," '\"',\n"," 'Nameless',\n"," 'penal',\n"," 'military',\n"," 'unit',\n"," 'serving',\n"," 'nation',\n"," 'Gallia',\n"," 'during',\n"," 'Second',\n"," 'Europan',\n"," 'War',\n"," 'who',\n"," 'perform',\n"," 'secret',\n"," 'black',\n"," 'operations',\n"," 'are',\n"," 'pitted',\n"," 'against',\n"," 'Imperial',\n"," 'Raven']\n","\n","{'\"': 4,\n"," '(': 1,\n"," ')': 1,\n"," ',': 6,\n"," '.': 4,\n"," '2011': 1,\n"," '3': 2,\n"," ':': 2,\n"," '<unk>': 3,\n"," '=': 2,\n"," '@-@': 2,\n"," 'Battlefield': 1,\n"," 'Chronicles': 3,\n"," 'Europan': 1,\n"," 'Gallia': 1,\n"," 'III': 2,\n"," 'Imperial': 1,\n"," 'January': 1,\n"," 'Japan': 2,\n"," 'Japanese': 1,\n"," 'Media.Vision': 1,\n"," 'Nameless': 1,\n"," 'PlayStation': 1,\n"," 'Portable': 1,\n"," 'Raven': 1,\n"," 'Released': 1,\n"," 'Second': 1,\n"," 'Sega': 1,\n"," 'Senjō': 1,\n"," 'Valkyria': 5,\n"," 'War': 1,\n"," 'a': 2,\n"," 'against': 1,\n"," 'and': 4,\n"," 'are': 1,\n"," 'as': 2,\n"," 'black': 1,\n"," 'by': 1,\n"," 'commonly': 1,\n"," 'developed': 1,\n"," 'during': 1,\n"," 'first': 1,\n"," 'follows': 1,\n"," 'for': 1,\n"," 'fusion': 1,\n"," 'game': 3,\n"," 'gameplay': 1,\n"," 'in': 3,\n"," 'is': 2,\n"," 'it': 1,\n"," 'its': 1,\n"," 'lit': 1,\n"," 'military': 1,\n"," 'nation': 1,\n"," 'no': 1,\n"," 'of': 3,\n"," 'operations': 1,\n"," 'outside': 1,\n"," 'parallel': 1,\n"," 'penal': 1,\n"," 'perform': 1,\n"," 'pitted': 1,\n"," 'playing': 1,\n"," 'predecessors': 1,\n"," 'real': 1,\n"," 'referred': 1,\n"," 'role': 1,\n"," 'runs': 1,\n"," 'same': 1,\n"," 'secret': 1,\n"," 'series': 1,\n"," 'serving': 1,\n"," 'story': 1,\n"," 'tactical': 2,\n"," 'the': 11,\n"," 'third': 1,\n"," 'time': 1,\n"," 'to': 2,\n"," 'unit': 2,\n"," 'video': 1,\n"," 'who': 1,\n"," '戦場のヴァルキュリア3': 1}\n","\n","132\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tDSNA9ccPbpn","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","class Corpus(object):\n","    def __init__(self, path):\n","        # We create an object Dictionary associated to Corpus\n","        self.dictionary = Dictionary()\n","        # We go through all files, adding all words to the dictionary\n","        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n","        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n","        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n","        \n","    def tokenize(self, path):\n","        \"\"\"Tokenizes a text file, knowing the dictionary, in order to tranform it into a list of indexes\"\"\"\n","        assert os.path.exists(path)\n","        # Add words to the dictionary\n","        with open(path, 'r') as f:\n","            tokens = 0\n","            for line in f:\n","                words = line.split() + ['<eos>']\n","                for word in words:\n","                    self.dictionary.add_word(word)\n","                tokens += len(words)\n","        \n","        # Once done, go through the file a second time and fill a Torch Tensor with the associated indexes \n","        \"\"\"\n","        A compléter\n","        \"\"\"\n","        #ids = torch.tensor(self.dictionary.total)\n","        tableau = np.zeros(self.dictionary.total, dtype=int)\n","        with open(path, 'r') as f:\n","            i = 0\n","            for line in f:\n","                words = line.split() + ['<eos>']\n","                for word in words:\n","                    idx = self.dictionary.word2idx[word] # on récupère l'index\n","                    tableau[i] = idx\n","                    i+=1\n","        ids = torch.tensor(tableau)\n","\n","        return ids"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zAIG8zbTPbp1","colab_type":"text"},"source":["We use the corpus (wikitext-2)[https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/]. While it's a small dataset, we will need to reduce it if we want to train a model on it without a gpu."]},{"cell_type":"code","metadata":{"id":"DQY2hDMWPbp3","colab_type":"code","colab":{}},"source":["###############################################################################\n","# Load data\n","###############################################################################\n","\n","data = './'\n","corpus = Corpus(data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"azBcQjK1Pbp_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":170},"outputId":"eaf4df64-e8d1-4dc1-d169-40c562f76f81","executionInfo":{"status":"ok","timestamp":1583749273329,"user_tz":-60,"elapsed":7126,"user":{"displayName":"Thomas KOCH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ87m3muD8rwgrNqbB_aT6SAfORhzKUeCrE4YIZA=s64","userId":"03343798980471844337"}}},"source":["#Examples and visualization\n","print(corpus.dictionary.total)\n","print(len(corpus.dictionary.idx2word))\n","print(len(corpus.dictionary.word2idx))\n","\n","print(corpus.train.shape)\n","print(corpus.train[0:7])\n","print([corpus.dictionary.idx2word[corpus.train[i]] for i in range(7)])\n","\n","print(corpus.valid.shape)\n","print(corpus.valid[0:7])\n","print([corpus.dictionary.idx2word[corpus.valid[i]] for i in range(7)])"],"execution_count":85,"outputs":[{"output_type":"stream","text":["2551843\n","33278\n","33278\n","torch.Size([2088628])\n","tensor([0, 1, 2, 3, 4, 1, 0])\n","['<eos>', '=', 'Valkyria', 'Chronicles', 'III', '=', '<eos>']\n","torch.Size([2306274])\n","tensor([    0,     1, 32966, 32967,     1,     0,     0])\n","['<eos>', '=', 'Homarus', 'gammarus', '=', '<eos>', '<eos>']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NezUgcE9PbqG","colab_type":"code","colab":{}},"source":["# We now have data under a very long list of indexes: the text is as one sequence.\n","# The idea now is to create batches from this. Note that this is absolutely not the best\n","# way to proceed with large quantities of data (where we'll try not to store huge tensors\n","# in memory but read them from file as we go) !\n","# Here, we are looking for simplicity and efficiency with regards to computation time.\n","# That is why we will ignore sentence separations and treat the data as one long stream that\n","# we will cut arbitrarily as we need.\n","# With the alphabet being our data, we currently have the sequence:\n","# [a b c d e f g h i j k l m n o p q r s t u v w x y z]\n","# We want to reorganize it as independant batches that will be processed independantly by the model !\n","# For instance, with the alphabet as the sequence and batch size 4, we'd get the 4 following sequences:\n","# ┌ a g m s ┐\n","# │ b h n t │\n","# │ c i o u │\n","# │ d j p v │\n","# │ e k q w │\n","# └ f l r x ┘\n","# with the last two elements being lost.\n","# Again, these columns are treated as independent by the model, which means that the\n","# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient processing.\n","\n","def batchify(data, batch_size, cuda = True):\n","    # Cut the elements that are unnecessary\n","    \"\"\"\n","    A compléter\n","    \"\"\"\n","    data = data[:len(data)-len(data)%batch_size]\n","    \n","    # Reorganize the data\n","    \"\"\"\n","    A compléter\n","    \"\"\"\n","    data = data.view(-1, batch_size) # le -1 permet de donner directement la bonne dimension en fonction des autres\n","\n","    # If we can use a GPU, let's tranfer the tensor to it\n","    if cuda:\n","        data = data.cuda()\n","    return data\n","\n","# get_batch subdivides the source data into chunks of the appropriate length.\n","# If source is equal to the example output of the batchify function, with\n","# a sequence length (seq_len) of 3, we'd get the following two variables:\n","# ┌ a g m s ┐ ┌ b h n t ┐\n","# | b h n t | | c i o u │\n","# └ c i o u ┘ └ d j p v ┘\n","# The first variable contains the letters input to the network, while the second\n","# contains the one we want the network to predict (b for a, h for g, v for u, etc..)\n","# Note that despite the name of the function, we are cutting the data in the\n","# temporal dimension, since we already divided data into batches in the previous\n","# function. \n","\n","def get_batch(source, i, seq_len, evaluation=False):\n","    # Deal with the possibility that there's not enough data left for a full sequence\n","    # Take the input data - shift by one for the target data\n","\n","    \"\"\"\n","    A compléter\n","    \"\"\"\n","    long = len(source)\n","    data = source[i:min(long, i+seq_len)]\n","    target = source[min(long,i+1):min(long,i+1+seq_len)]\n","\n","\n","    return data, target"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RDPnm9_lPbqO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"8b89ec5c-46fb-4b53-bf2b-64bac57a576b","executionInfo":{"status":"ok","timestamp":1583749273335,"user_tz":-60,"elapsed":7039,"user":{"displayName":"Thomas KOCH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ87m3muD8rwgrNqbB_aT6SAfORhzKUeCrE4YIZA=s64","userId":"03343798980471844337"}}},"source":["#Examples and visualization\n","batch_size = 100\n","eval_batch_size = 4\n","train_data = batchify(corpus.train, batch_size)\n","val_data = batchify(corpus.valid, eval_batch_size)\n","test_data = batchify(corpus.test, eval_batch_size)\n","\n","print(train_data.shape)\n","print(val_data.shape)"],"execution_count":87,"outputs":[{"output_type":"stream","text":["torch.Size([20886, 100])\n","torch.Size([576568, 4])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YpsCDl8WPbqT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"outputId":"2ddd0ed0-2a4e-4f9b-89ea-1f5a2e61d5ea","executionInfo":{"status":"ok","timestamp":1583749273340,"user_tz":-60,"elapsed":6993,"user":{"displayName":"Thomas KOCH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ87m3muD8rwgrNqbB_aT6SAfORhzKUeCrE4YIZA=s64","userId":"03343798980471844337"}}},"source":["#Examples and visualization\n","input_words, target_words = get_batch(val_data, 0, 3)\n","pp.pprint(input_words)\n","pp.pprint(target_words)\n","input_words, target_words = get_batch(val_data, 3, 3)\n","pp.pprint(input_words)\n","pp.pprint(target_words)"],"execution_count":88,"outputs":[{"output_type":"stream","text":["tensor([[    0,     1, 32966, 32967],\n","        [    1,     0,     0, 32966],\n","        [32967,    13,   406,    23]], device='cuda:0')\n","tensor([[    1,     0,     0, 32966],\n","        [32967,    13,   406,    23],\n","        [   17,  6253, 19902,   310]], device='cuda:0')\n","tensor([[   17,  6253, 19902,   310],\n","        [ 1444, 19902,    13,    26],\n","        [   27,  2576,    16,     9]], device='cuda:0')\n","tensor([[ 1444, 19902,    13,    26],\n","        [   27,  2576,    16,     9],\n","        [19902,   115,    17,  4929]], device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cfPLesdiPbqa","colab_type":"text"},"source":["### LSTM Cells in pytorch"]},{"cell_type":"code","metadata":{"id":"Ep5xjtxqPbqc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"2486ae50-599d-4e63-df40-ec7616f0b96e","executionInfo":{"status":"ok","timestamp":1583749273342,"user_tz":-60,"elapsed":6944,"user":{"displayName":"Thomas KOCH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQ87m3muD8rwgrNqbB_aT6SAfORhzKUeCrE4YIZA=s64","userId":"03343798980471844337"}}},"source":["# Create a toy example of LSTM: \n","lstm = nn.LSTM(3, 3)  # Input dim is 3, output dim is 3\n","inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5\n","\n","# LSTMs expect inputs having 3 dimensions:\n","# - The first dimension is the temporal dimension, along which we (in our case) have the different words\n","# - The second dimension is the batch dimension, along which we stack the independant batches\n","# - The third dimension is the feature dimension, along which are the features of the vector representing the words\n","\n","# In our toy case, we have inputs and outputs containing 3 features (third dimension !)\n","# We created a sequence of 5 different inputs (first dimension !)\n","# We don't use batch (the second dimension will have one lement)\n","\n","# We need an initial hidden state, of the right sizes for dimension 2/3, but with only one temporal element:\n","# Here, it is:\n","hidden = (torch.randn(1, 1, 3),\n","          torch.randn(1, 1, 3))\n","# Why do we create a tuple of two tensors ? Because we use LSTMs: remember that they use two sets of weights,\n","# and two hidden states (Hidden state, and Cell state).\n","# If you don't remember, read: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n","# If we used a classic RNN, we would simply have:\n","# hidden = torch.randn(1, 1, 3)\n","\n","# The naive way of applying a lstm to inputs is to apply it one step at a time, and loop through the sequence\n","for i in inputs:\n","    # After each step, hidden contains the hidden states (remember, it's a tuple of two states).\n","    out, hidden = lstm(i.view(1, 1, -1), hidden)\n","    \n","# Alternatively, we can do the entire sequence all at once.\n","# The first value returned by LSTM is all of the Hidden states throughout the sequence.\n","# The second is just the most recent Hidden state and Cell state (you can compare the values)\n","# The reason for this is that:\n","# \"out\" will give you access to all hidden states in the sequence, for each temporal step\n","# \"hidden\" will allow you to continue the sequence and backpropagate later, with another sequence\n","inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n","hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # Re-initialize\n","out, hidden = lstm(inputs, hidden)\n","pp.pprint(out)\n","pp.pprint(hidden)"],"execution_count":89,"outputs":[{"output_type":"stream","text":["tensor([[[-2.2452e-01,  5.4786e-02, -3.3949e-02]],\n","\n","        [[-6.9429e-02, -1.9264e-02, -6.1983e-02]],\n","\n","        [[ 3.2582e-01, -1.1988e-01, -1.1599e-04]],\n","\n","        [[ 3.1922e-01, -1.7229e-01, -3.3374e-02]],\n","\n","        [[ 3.3516e-01, -8.9476e-02,  1.0184e-02]]], grad_fn=<StackBackward>)\n","(tensor([[[ 0.3352, -0.0895,  0.0102]]], grad_fn=<StackBackward>),\n"," tensor([[[ 0.4893, -0.8318,  0.0454]]], grad_fn=<StackBackward>))\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"s_lJyAsPPbqj","colab_type":"text"},"source":["### Creating our own LSTM Model"]},{"cell_type":"code","metadata":{"id":"O0kGCwslPbql","colab_type":"code","colab":{}},"source":["# Models are usually implemented as custom nn.Module subclass\n","# We need to redefine the __init__ method, which creates the object\n","# We also need to redefine the forward method, which transform the input into outputs\n","# We can also add any method that we need: here, in order to initiate weights in the model\n","#   ntokens = size_voc\n","#   ninp = size_embeddings\n","#   nhid = size_hidden\n","#   nlayers = layers\n","\n","\n","class LSTMModel(nn.Module):\n","    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n","        super(LSTMModel, self).__init__()\n","        # Create a dropout object to use on layers for regularization\n","        \"\"\"\n","        A compléter\n","        \"\"\"\n","        self.dropout = nn.Dropout(dropout) \n","\n","        # Create an encoder - which is an embedding layer\n","        \"\"\"\n","        A compléter\n","        \"\"\"\n","        self.encoder = nn.Embedding(ntoken, ninp) # nn.Embedding(vocab_size, embedding_dim)\n","\n","        # Create the LSTM layers - find out how to stack them !\n","        \"\"\"\n","        A compléter\n","        \"\"\"\n","        self.lstm = nn.LSTM(ninp, nhid, nlayers, dropout=dropout) # nn.LSTM(embedding_dim, hidden_dim)\n","\n","        # Create what we call the decoder: a linear transformation to map the hidden state into scores for all words in the vocabulary\n","        # (Note that the softmax application function will be applied out of the model)\n","        \"\"\"\n","        A compléter\n","        \"\"\"\n","        self.decoder =  nn.Linear(nhid, ntoken) # nn.Linear(hidden_dim, tagset_size)\n","        \n","        # Initialize non-reccurent weights \n","        self.init_weights()\n","\n","        self.ninp = ninp\n","        self.nhid = nhid\n","        self.nlayers = nlayers\n","        \n","    def init_weights(self):\n","        # Initialize the encoder and decoder weights with the uniform distribution\n","        initrange = 0.1\n","        self.encoder.weight.data.uniform_(-initrange, initrange)\n","        self.decoder.bias.data.fill_(0)\n","        self.decoder.weight.data.uniform_(-initrange, initrange)\n","        \n","    def init_hidden(self, batch_size):\n","        # Initialize the hidden state and cell state to zero, with the right sizes\n","        weight = next(self.parameters()).data\n","        \"\"\"\n","        A compléter\n","        \"\"\"\n","        hidden = Variable(weight.new(self.nlayers, batch_size, self.nhid).zero_())\n","        return (hidden, hidden)      \n","\n","    def forward(self, input, hidden, return_h=False):\n","        # Process the input\n","        \"\"\"\n","        A compléter\n","        \"\"\"\n","        emb = self.drop(self.encoder(input))\n","\n","        # Apply the LSTMs\n","        \"\"\"\n","        A compléter\n","        \"\"\"\n","        output, hidden = self.lstm(emb, hidden)\n","        output = self.dropt(output)\n","        s = output.size()\n","\n","        # Decode into scores\n","        \"\"\"\n","        A compléter\n","        \"\"\"\n","        decoded = self.decoder(output.view(s[0]*s[1], s[2]))\n","        decoded = decoded.view(s[0], s[1], decoded.size(1))\n","\n","        return decoded, hidden\n","\n","\n","    def forward(self, sentence):\n","        embeds = self.word_embeddings(sentence)\n","        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n","        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n","        tag_scores = F.log_softmax(tag_space, dim=1)\n","        return tag_scores"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kp6tbVUtPbqr","colab_type":"text"},"source":["### Building the Model"]},{"cell_type":"code","metadata":{"id":"i9lcBqHFPbqt","colab_type":"code","colab":{}},"source":["# Set the random seed manually for reproducibility.\n","torch.manual_seed(1)\n","\n","# If you have Cuda installed and a GPU available\n","cuda = True\n","if torch.cuda.is_available():\n","    if not cuda:\n","        print(\"WARNING: You have a CUDA device, so you should probably choose cuda = True\")\n","        \n","device = torch.device(\"cuda\" if cuda else \"cpu\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JYpYF2dxPbq1","colab_type":"code","colab":{}},"source":["embedding_size = 200\n","hidden_size = 200\n","layers = 2\n","dropout = 0.5\n","\n","###############################################################################\n","# Build the model\n","###############################################################################\n","\n","vocab_size = len(corpus.dictionary) # A compléter\n","model = LSTMModel(ntoken=vocab_size, ninp=embedding_size, nhid=hidden_size, nlayers=layers, dropout=0.5)# A compléter \n","model = model.to(device)\n","params = list(model.parameters())\n","criterion = nn.CrossEntropyLoss()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xadyW-NkPbq8","colab_type":"code","colab":{}},"source":["lr = 10.0\n","optimizer = 'sgd'\n","wdecay = 1.2e-6\n","# For gradient clipping\n","clip = 0.25\n","\n","# Create the optimizer\n","optim = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=wdecay, ) # A compléter"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kv1qaAtMPbrD","colab_type":"code","colab":{}},"source":["# Let's think about gradient propagation:\n","# We plan to keep the second ouput of the LSTM layer (the hidden/cell states) to initialize\n","# the next call to LSTM. In this way, we can back-propagate the gradient for as long as we want.\n","# However, this put a huge strain on the memory used by the model, since it implies retaining\n","# a always-growing number of tensors of gradients in the cache.\n","# We decide to not backpropagate through time beyond the current sequence ! \n","# We use a specific function to cut the 'hidden/state cell' states from their previous dependencies\n","# before using them to initialize the next call to the LSTM.\n","# This is done with the .detach() function.\n","\n","def repackage_hidden(h):\n","    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n","    if isinstance(h, torch.Tensor):\n","        return # A compléter\n","    else:\n","        return tuple(repackage_hidden(v) for v in h)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cm1d996cPbrL","colab_type":"code","colab":{}},"source":["# Other global parameters\n","epochs = 10\n","seq_len = 30\n","log_interval = 10\n","save = 'model.pt'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W7l_N6GpPbrR","colab_type":"code","colab":{}},"source":["def train():\n","    # Turn on training mode which enables dropout.\n","    model.train()\n","    total_loss = 0.\n","    start_time = time.time()\n","    # Initialize the hidden/cell state\n","    \"\"\"\n","    A compléter\n","    \"\"\"\n","    for batch, i in enumerate(range(0, train_data.size(0) - 1, seq_len)):\n","        # Get the input/target data\n","        \"\"\"\n","        A compléter\n","        \"\"\"\n","        # Starting each batch, we detach the hidden state from how it was previously produced.\n","        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n","        \"\"\"\n","        A compléter\n","        \"\"\"\n","        # Do the training loop: careful, look into the documentation for the criterion CrossEntropyLoss()\n","        \"\"\"\n","        A compléter\n","        \"\"\"\n","        # Do the gradient clipping with the function torch.nn.utils.clip_grad_norm_, then the optimization step\n","        \"\"\"\n","        A compléter\n","        \"\"\"        \n","        # We use .data to only accumulate the loss, and not keep track of the gradient too\n","        \"\"\"\n","        A compléter\n","        \"\"\" \n","        if batch % log_interval == 0 and batch > 0:\n","            cur_loss = total_loss / log_interval\n","            elapsed = time.time() - start_time\n","            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n","                    'loss {:5.2f} | ppl {:8.2f}'.format(\n","                epoch, batch, len(train_data) // seq_len, lr,\n","                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n","            total_loss = 0\n","            start_time = time.time()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"C53WC5-CPbra","colab_type":"code","colab":{}},"source":["def evaluate(data_source):\n","    # Turn on evaluation mode which disables dropout.\n","    model.eval()\n","    total_loss = 0.\n","    # Initialize the hidden/cell state\n","    \"\"\"\n","    A compléter\n","    \"\"\"\n","    with torch.no_grad():\n","        for i in range(0, data_source.size(0) - 1, seq_len):\n","            \"\"\"\n","            A compléter\n","            \"\"\"\n","    return total_loss / (len(data_source) - 1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TRRnfDy7Pbrg","colab_type":"code","colab":{}},"source":["# Loop over epochs.\n","best_val_loss = None\n","\n","# At any point you can hit Ctrl + C to break out of training early.\n","try:\n","    for epoch in range(1, epochs+1):\n","        epoch_start_time = time.time()\n","        train()\n","        val_loss = evaluate(val_data)\n","        print('-' * 89)\n","        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n","                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n","                                           val_loss, math.exp(val_loss)))\n","        print('-' * 89)\n","        # Save the model if the validation loss is the best we've seen so far.\n","        if not best_val_loss or val_loss < best_val_loss:\n","            with open(save, 'wb') as f:\n","                torch.save(model, f)\n","            best_val_loss = val_loss\n","        else:\n","            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n","            lr /= 4.0\n","except KeyboardInterrupt:\n","    print('-' * 89)\n","    print('Exiting from training early')\n","\n","# Load the best saved model.\n","with open(save, 'rb') as f:\n","    model = torch.load(f)\n","    # After loading, the parameters are not a continuous chunk of memory\n","    # This makes them a continuous chunk, and will speed up the forward pass\n","    model.rnn.flatten_parameters()\n","\n","# Run on test data.\n","test_loss = evaluate(test_data)\n","print('=' * 89)\n","print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n","    test_loss, math.exp(test_loss)))\n","print('=' * 89)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eRA2Bc-YPbrn","colab_type":"code","colab":{}},"source":["# Implement a function to generate a fixed number of words given an input text\n","# You can choose the next word by doing multinomial sampling from the output distribution (which you can control using softmax temperature) \n","# or simply using the argmax."],"execution_count":0,"outputs":[]}]}