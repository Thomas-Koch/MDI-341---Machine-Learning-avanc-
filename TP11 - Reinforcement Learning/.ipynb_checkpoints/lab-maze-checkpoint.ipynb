{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates the concepts and algorithms of reinforcement learning to esc a maze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import display_maze, display_policy, display_value, display_diff_policy, normalize_sparse, get_moves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = sparse.load_npz('maze.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1.5\n",
    "display_maze(maze, scale = scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_terminal = [(1,20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    \"\"\"\n",
    "    Attributes\n",
    "    ----------\n",
    "    maze: sparse csr matrix\n",
    "        Binary map of the maze\n",
    "    states: list of tuples\n",
    "        States\n",
    "    states_terminal: list of tuples\n",
    "        Terminal states\n",
    "    state_id: dict\n",
    "        Id of each state\n",
    "    terminals: numpy array\n",
    "        Ids of terminal states        \n",
    "    actives: numpy array\n",
    "        Ids of non terminal states\n",
    "    adjacency: sparse csr matrix\n",
    "        Matrix of adjacent ids \n",
    "    rewards: numpy array\n",
    "        Reward of each state\n",
    "    gamma: float (default = 1)\n",
    "        Discount factor\n",
    "    \"\"\"\n",
    "    def __init__(self, maze, states_terminal = None, gamma = 1):\n",
    "        n = maze.nnz\n",
    "        maze_coo = sparse.coo_matrix(maze)\n",
    "        states = [(maze_coo.row[i], maze_coo.col[i]) for i in range(n)]\n",
    "\n",
    "        if states_terminal is None:\n",
    "            states_terminal = [np.random.choice(states)]\n",
    "\n",
    "        state_id = {s:i for i, s in enumerate(states)}\n",
    "\n",
    "        terminals = np.array([state_id[s] for s in states_terminal])\n",
    "        actives = np.setdiff1d(np.arange(n), terminals)\n",
    "\n",
    "        row = []\n",
    "        col = []\n",
    "        for (i,j) in state_id:\n",
    "            if (i + 1, j) in state_id:\n",
    "                row.append(state_id[(i, j)])\n",
    "                col.append(state_id[(i + 1, j)])\n",
    "            if (i, j + 1) in state_id:\n",
    "                row.append(state_id[(i, j)])\n",
    "                col.append(state_id[(i, j + 1)])\n",
    "        adjacency = sparse.csr_matrix((np.ones_like(row), (row, col)), shape = (n, n))   \n",
    "        \n",
    "        rewards = -np.ones(n)\n",
    "        \n",
    "        self.maze = maze\n",
    "        self.states = states\n",
    "        self.states_terminal = states_terminal\n",
    "        self.state_id = state_id\n",
    "        self.terminals = terminals\n",
    "        self.actives = actives\n",
    "        self.adjacency = adjacency + adjacency.T\n",
    "        self.rewards = rewards\n",
    "        self.gamma = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(maze, states_terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.actives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The policy is defined as a sparse transition matrix between states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random policy\n",
    "policy = normalize_sparse(model.adjacency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, V = None, model = model, n_iter_eval = 1000):\n",
    "    \"\"\"Evaluate a policy by iterations, starting from V\"\"\"\n",
    "    rewards = model.rewards\n",
    "    gamma = model.gamma\n",
    "    terminals = model.terminals\n",
    "    transition = policy\n",
    "    if V is None:\n",
    "        V = np.zeros_like(rewards)\n",
    "    for t in range(n_iter_eval):\n",
    "        V = transition.dot(rewards + gamma * V)\n",
    "        V[terminals] = 0\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy_from_value(V, model = model):\n",
    "    \"\"\"Get the greedy policy associated with V\"\"\"\n",
    "    n = len(model.states)\n",
    "    actives = model.actives\n",
    "    rewards = model.rewards\n",
    "    gamma = model.gamma\n",
    "    row = []\n",
    "    col = []\n",
    "    for i in actives:\n",
    "        indices = model.adjacency[i].indices\n",
    "        values = (rewards + gamma * V)[indices]\n",
    "        j = indices[np.argmax(values)]\n",
    "        row.append(i)\n",
    "        col.append(j)\n",
    "    policy = normalize_sparse(sparse.csr_matrix((np.ones_like(row), (row, col)), shape = (n, n)))\n",
    "    return policy    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_policy(policy, model = model, n_iter_max = 100, n_iter_eval = 1000, verbose = True):\n",
    "    \"\"\"Get the optimal policy by policy iteration\"\"\"\n",
    "    V = None\n",
    "    policy_prev = sparse.csr_matrix(policy.shape)\n",
    "    t = 0\n",
    "    while (policy - policy_prev).nnz and t < n_iter_max:\n",
    "        V = evaluate_policy(policy, V, model, n_iter_eval)\n",
    "        policy_prev = policy.copy()\n",
    "        policy = get_policy_from_value(V, model)\n",
    "        t += 1\n",
    "    if verbose:\n",
    "        print(\"Number of iterations =\", t - 1)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = iterate_policy(policy)\n",
    "V = evaluate_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display_value(model.maze, V, scale = scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moves = get_moves(policy, model)\n",
    "display_policy(model.maze, moves, scale = scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial values\n",
    "V = np.zeros(len(model.states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_value(V, model = model, n_iter = 100, verbose = True):\n",
    "    \"\"\"Get the optimal policy by value iteration\"\"\"\n",
    "    policy = get_policy_from_value(V, model)\n",
    "    for t in range(n_iter):\n",
    "        V = evaluate_policy(policy, V, model, 1)\n",
    "        policy = get_policy_from_value(V, model)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = iterate_value(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = evaluate_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_value(model.maze, V, scale = scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moves = get_moves(policy, model)\n",
    "display_policy(model.maze, moves, scale = scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial values of Q\n",
    "Q = normalize_sparse(model.adjacency)\n",
    "Q.data = np.zeros_like(Q.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action is identified with new state (after move)\n",
    "\n",
    "def sarsa(Q, model = model, alpha = 0.1, eps = 0.1, n_iter = 100):\n",
    "    terminals = model.terminals\n",
    "    rewards = model.rewards\n",
    "    gamma = model.gamma\n",
    "    # random state (not terminal)\n",
    "    state = np.random.choice(np.setdiff1d(np.arange(len(model.states)), terminals))\n",
    "    # random action\n",
    "    action = np.random.choice(Q[state].indices)\n",
    "    new_state = action\n",
    "    for t in range(n_iter):\n",
    "        state_prev = state\n",
    "        action_prev = action\n",
    "        state = new_state\n",
    "        if state in terminals:\n",
    "            # restart\n",
    "            state = np.random.choice(np.setdiff1d(np.arange(len(model.states)), terminals))\n",
    "            action = np.random.choice(Q[state].indices)\n",
    "            Q[state_prev, action_prev] = (1 - alpha) *  Q[state_prev, action_prev] + alpha * rewards[action_prev]\n",
    "        else:\n",
    "            best_action = Q[state].indices[np.argmax(Q[state].data)]\n",
    "            if np.random.random() < eps:\n",
    "                action = np.random.choice(Q[state].indices)\n",
    "            else:\n",
    "                action = best_action\n",
    "            Q[state_prev, action_prev] = (1 - alpha) *  Q[state_prev, action_prev] + alpha * (rewards[action_prev] + gamma * Q[state, action])\n",
    "        new_state = action\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy_from_action_value(Q, model = model):\n",
    "    \"\"\"Get the greedy policy associated with Q\"\"\"\n",
    "    n = len(model.states)\n",
    "    row = []\n",
    "    col = []\n",
    "    for i in range(n):\n",
    "        indices = Q[i].indices\n",
    "        j = indices[np.argmax(Q[i].data)]\n",
    "        row.append(i)\n",
    "        col.append(j)\n",
    "    policy = normalize_sparse(sparse.csr_matrix((np.ones_like(row), (row, col)), shape = (n, n)))\n",
    "    return policy    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = sarsa(Q, n_iter = n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = get_policy_from_action_value(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moves = get_moves(policy, model)\n",
    "display_policy(model.maze, moves, scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
