{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vol d'hélicoptère"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook montre quelques algorithmes d'apprentissage par renforcement appliqués à un vol d'hélicoptère en 2D (distance, altitude) :\n",
    "* Value Iteration\n",
    "* Policy Iteration\n",
    "* SARSA\n",
    "* Q-learning\n",
    "\n",
    "Une pénalisation peut être appliquée pour un vol à basse altitude.\n",
    "\n",
    "Votre mission est de tester les différents algorithmes et leurs paramètres, puis d'implémenter l'algorithme Q($\\lambda$) utilisant la trace des trajectoires, avec un paramètre de trace $\\lambda \\in [0,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = 10\n",
    "WIDTH = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_REWARD = 100\n",
    "WORST_REWARD = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(state, high_altitude = 0, high_alt_cost = 1, low_alt_cost = 10):\n",
    "    reward = 0\n",
    "    if state[0] < 0:\n",
    "        if state[1] == WIDTH - 1:\n",
    "            # arrival\n",
    "            reward = BEST_REWARD\n",
    "        else: \n",
    "            # crash\n",
    "            reward = WORST_REWARD\n",
    "    elif state[0] < high_altitude:\n",
    "        # penalize low altitude\n",
    "        reward = -low_alt_cost\n",
    "    else:\n",
    "        reward = -high_alt_cost\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [(1,0), (-1,0), (0,1), (0,-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [(i,j) for i in range(HEIGHT) for j in range(WIDTH)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_state(state):\n",
    "    return (state[0] >=0) and (state[0] < HEIGHT) and (state[1] >=0) and (state[1] < WIDTH) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terminal_state(state):\n",
    "    return (state[0] < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_action(state, action):\n",
    "    new_state = move(state, action)\n",
    "    return valid_state(new_state) or terminal_state(new_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move(state, action):\n",
    "    return tuple(np.array(state) + np.array(action))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_value(V, cmap = \"jet\", scale = 20):\n",
    "    max_dimension = max(HEIGHT, WIDTH)\n",
    "    plt.figure(figsize=(scale * HEIGHT / max_dimension,scale * WIDTH / max_dimension))\n",
    "    plt.imshow(np.flip(V, axis = 0), cmap, vmin = WORST_REWARD, vmax = BEST_REWARD)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration_synchronous(gamma, tol, max_iter):\n",
    "    '''\n",
    "    gamma: float\n",
    "        discount factor\n",
    "    tol: float\n",
    "        tolerance for convergence\n",
    "    max_iter: float\n",
    "        max number of iterations\n",
    "    '''\n",
    "    V = np.zeros((HEIGHT, WIDTH))\n",
    "    error = np.inf\n",
    "    iteration = 0\n",
    "    while error > tol:\n",
    "        new_V = np.zeros((HEIGHT, WIDTH))\n",
    "        for state in states:\n",
    "            max_value = np.NINF\n",
    "            for action in actions:\n",
    "                new_state = move(state, action)\n",
    "                if valid_state(new_state):\n",
    "                    value = get_reward(new_state) + gamma * V[new_state]\n",
    "                elif terminal_state(new_state):\n",
    "                    value = get_reward(new_state)\n",
    "                else:\n",
    "                    value = np.NINF\n",
    "                if value > max_value:\n",
    "                    max_value = value\n",
    "            new_V[state] = max_value\n",
    "        error = np.max(np.abs(new_V - V))\n",
    "        V = new_V\n",
    "        iteration += 1\n",
    "        if iteration >= max_iter:\n",
    "            break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration_asynchronous(gamma, tol, max_iter):\n",
    "    '''\n",
    "    gamma: float\n",
    "        discount factor\n",
    "    tol: float\n",
    "        tolerance for convergence\n",
    "    max_iter: float\n",
    "        max number of iterations\n",
    "    '''\n",
    "    V = np.zeros((HEIGHT, WIDTH))\n",
    "    error = np.inf\n",
    "    iteration = 0\n",
    "    while error > tol:\n",
    "        for state in states:\n",
    "            max_value = np.NINF\n",
    "            for action in actions:\n",
    "                new_state = move(state, action)\n",
    "                if valid_state(new_state):\n",
    "                    value = get_reward(new_state) + gamma * V[new_state]\n",
    "                elif terminal_state(new_state):\n",
    "                    value = get_reward(new_state)\n",
    "                else:\n",
    "                    value = np.NINF\n",
    "                if value > max_value:\n",
    "                    max_value = value\n",
    "            error_ = np.abs(max_value - V[state])\n",
    "            if error_ > error:\n",
    "                error = error_\n",
    "            V[state] = max_value\n",
    "        iteration += 1\n",
    "        if iteration >= max_iter:\n",
    "            break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(asynchronous = True, gamma = 1, tol = 1e-2, max_iter = 50):\n",
    "    '''\n",
    "    asynchronous: bool\n",
    "        if True, asynchronous updates\n",
    "    gamma: float\n",
    "        discount factor\n",
    "    tol: float\n",
    "        tolerance for convergence\n",
    "    max_iter: float\n",
    "        max number of iterations\n",
    "    '''\n",
    "    if asynchronous:\n",
    "        return value_iteration_asynchronous(gamma, tol, max_iter)\n",
    "    else:\n",
    "        return value_iteration_synchronous(gamma, tol, max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = np.zeros((HEIGHT, WIDTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_value(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "V = value_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_value(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_policy(V, gamma = 1):\n",
    "    policy = np.zeros((HEIGHT, WIDTH), dtype=tuple)\n",
    "    for state in states:\n",
    "        max_value = np.NINF\n",
    "        for action in actions:\n",
    "            new_state = move(state, action)\n",
    "            if valid_state(new_state):\n",
    "                value = get_reward(new_state) + gamma * V[new_state]\n",
    "            elif terminal_state(new_state):\n",
    "                value = get_reward(new_state)\n",
    "            else:\n",
    "                value = np.NINF\n",
    "            if value > max_value:\n",
    "                max_value = value\n",
    "                best_action = action\n",
    "        policy[state] = best_action\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = best_policy(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(policy, state = (0, 0)):\n",
    "    path = []\n",
    "    while not(terminal_state(state)) and state not in path:\n",
    "        path.append(state)\n",
    "        action = policy[state]\n",
    "        state = move(state, action)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = get_path(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_path(path, scale = 20, cmap = \"Greys\"):\n",
    "    max_dimension = max(HEIGHT, WIDTH)\n",
    "    plt.figure(figsize=(scale * HEIGHT / max_dimension,scale * WIDTH / max_dimension))\n",
    "    P = np.zeros((HEIGHT, WIDTH))\n",
    "    for i,j in path:\n",
    "        P[i,j] = 1\n",
    "    plt.imshow(np.flip(P, axis = 0),cmap, vmin = 0, vmax = 1)\n",
    "    plt.xticks([], [])\n",
    "    plt.yticks([], [])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_path(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_random_policy():\n",
    "    policy = np.zeros((HEIGHT, WIDTH), dtype=tuple)\n",
    "    for state in states:\n",
    "        valid_actions = [action for action in actions if valid_action(state, action)]\n",
    "        policy[state] = valid_actions[np.random.choice(len(valid_actions))]\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation_synchronous(policy, gamma, tol, max_iter):\n",
    "    '''\n",
    "    gamma: float\n",
    "        discount factor\n",
    "    tol: float\n",
    "        tolerance for convergence\n",
    "    max_iter: float\n",
    "        max number of iterations\n",
    "    '''\n",
    "    V = np.zeros((HEIGHT, WIDTH))\n",
    "    error = np.inf\n",
    "    iteration = 0\n",
    "    while error > tol:\n",
    "        new_V = np.zeros((HEIGHT, WIDTH))\n",
    "        for state in states:\n",
    "            action = policy[state]\n",
    "            new_state = move(state, action)\n",
    "            if valid_state(new_state):\n",
    "                value = get_reward(new_state) + gamma * V[new_state]\n",
    "            elif terminal_state(new_state):\n",
    "                value = get_reward(new_state)\n",
    "            new_V[state] = value\n",
    "        error = np.max(np.abs(new_V - V))\n",
    "        V = new_V\n",
    "        iteration += 1\n",
    "        if iteration >= max_iter:\n",
    "            break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation_asynchronous(policy, gamma, tol, max_iter):\n",
    "    '''\n",
    "    gamma: float\n",
    "        discount factor\n",
    "    tol: float\n",
    "        tolerance for convergence\n",
    "    max_iter: float\n",
    "        max number of iterations\n",
    "    '''\n",
    "    V = np.zeros((HEIGHT, WIDTH))\n",
    "    error = np.inf\n",
    "    iteration = 0\n",
    "    while error > tol:\n",
    "        for state in states:\n",
    "            action = policy[state]\n",
    "            new_state = move(state, action)\n",
    "            if valid_state(new_state):\n",
    "                value = get_reward(new_state) + gamma * V[new_state]\n",
    "            elif terminal_state(new_state):\n",
    "                value = get_reward(new_state)\n",
    "            error_ = np.abs(value - V[state])\n",
    "            if error_ > error:\n",
    "                error = error_\n",
    "            V[state] = value\n",
    "        iteration += 1\n",
    "        if iteration >= max_iter:\n",
    "            break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, asynchronous = True, gamma = 1, tol = 1e-2, max_iter = 50):\n",
    "    '''\n",
    "    asynchronous: bool\n",
    "        if True, asynchronous updates\n",
    "    gamma: float\n",
    "        discount factor\n",
    "    tol: float\n",
    "        tolerance for convergence\n",
    "    max_iter: float\n",
    "        max number of iterations\n",
    "    '''\n",
    "    if asynchronous:\n",
    "        return policy_evaluation_asynchronous(policy, gamma, tol, max_iter)\n",
    "    else:\n",
    "        return policy_evaluation_synchronous(policy, gamma, tol, max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "policy = init_random_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_path(get_path(policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "V = policy_evaluation(policy)\n",
    "show_value(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "policy = best_policy(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_path(get_path(policy, state = (0,15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy iteration\n",
    "\n",
    "for i in range(20):\n",
    "    V = policy_evaluation(policy)\n",
    "    policy = best_policy(V)\n",
    "    \n",
    "show_path(get_path(policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online learning. On-policy (behaviour = target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_actions = [(state, action) for state in states for action in actions\n",
    "                 if valid_action(state, action)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = {(state, action): 0 for (state, action) in state_actions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(state, policy, eps = 0.1):\n",
    "    '''\n",
    "    eps: float\n",
    "        Parameter of the epsilon-greedy policy\n",
    "        Controls the exploration (eps = 0 means no exploration)\n",
    "    '''\n",
    "    if np.random.random() < eps:\n",
    "        valid_actions = [action for action in actions if valid_action(state, action)]\n",
    "        return valid_actions[np.random.choice(len(valid_actions))]\n",
    "    else:\n",
    "        return policy[state]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(alpha = 0.9, gamma = 1, max_sequence = 200, steps = 10000, \n",
    "          verbose = True, batch = 10**3):\n",
    "    policy = init_random_policy()\n",
    "    Q = {(state, action): 0 for (state, action) in state_actions}\n",
    "    for t in range(steps):\n",
    "        if verbose:\n",
    "            if t%batch == 0:\n",
    "                print('Batch ',str(t // batch + 1),' over ',str(steps // batch))\n",
    "        # start an episode\n",
    "        state = states[np.random.choice(len(states))]\n",
    "        action = get_action(state, policy)\n",
    "        sequence = []\n",
    "        while not(terminal_state(state)) and len(sequence) < max_sequence:\n",
    "            sequence.append(state)\n",
    "            new_state = move(state, action)\n",
    "            reward = get_reward(new_state)\n",
    "            new_action = get_action(new_state, policy)\n",
    "            if terminal_state(new_state):\n",
    "                Q[(state, action)] += alpha * (reward - Q[(state, action)])\n",
    "            else:\n",
    "                Q[(state, action)] += alpha * (reward + gamma * Q[(new_state, new_action)] \n",
    "                                               - Q[(state, action)])\n",
    "            state = new_state\n",
    "            action = new_action\n",
    "        # update policy\n",
    "        for state in sequence:\n",
    "            qvalues = {a: Q[(state, a)] for a in actions if valid_action(state, a)}\n",
    "            policy[state] = max(qvalues, key = qvalues.get)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sarsa_policy = sarsa(gamma = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = policy_evaluation(sarsa_policy, gamma=0.9)\n",
    "show_value(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_path(get_path(sarsa_policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online learning. Off-policy (behaviour $\\ne$ target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qlearning(alpha = 0.9, gamma = 1, max_sequence = 100, steps = 10**4, \n",
    "              verbose = True, batch = 10**3):\n",
    "    policy = init_random_policy()\n",
    "    Q = {(state, action): 0 for (state, action) in state_actions}\n",
    "    for t in range(steps):\n",
    "        if verbose:\n",
    "            if t%batch == 0:\n",
    "                print('Batch ',str(t // batch + 1),' over ',str(steps // batch))\n",
    "        # start an episode\n",
    "        state = states[np.random.choice(len(states))]\n",
    "        sequence = []\n",
    "        while not(terminal_state(state)) and len(sequence) < max_sequence:\n",
    "            sequence.append(state)\n",
    "            action = get_action(state, policy)\n",
    "            new_state = move(state, action)\n",
    "            reward = get_reward(new_state)\n",
    "            if terminal_state(new_state):\n",
    "                Q[(state, action)] += alpha * (reward - Q[(state, action)])\n",
    "            else:\n",
    "                qvalues = [Q[(new_state, a)] for a in actions if valid_action(new_state, a)]\n",
    "                Q[(state, action)] += alpha * (reward + gamma * np.max(np.array(qvalues))\n",
    "                                               - Q[(state, action)]) \n",
    "            state = new_state\n",
    "        # update policy\n",
    "        for state in sequence:\n",
    "            qvalues = {a: Q[(state, a)] for a in actions if valid_action(state, a)}\n",
    "            policy[state] = max(qvalues, key = qvalues.get)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "qlearning_policy = qlearning(gamma = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = policy_evaluation(qlearning_policy)\n",
    "show_value(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_path(get_path(qlearning_policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
